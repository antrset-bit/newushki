#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import time
import json
import pickle
import hashlib
import asyncio
import logging
import random
from pathlib import Path
from typing import List, Tuple

import numpy as np
import faiss

from fastapi import FastAPI, Request, Response
from telegram import Update, Document, ReplyKeyboardMarkup, InputFile
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters

import fitz  # PyMuPDF
from pdf2image import convert_from_path
import pytesseract
from docx import Document as DocxDocument

from google import genai
from google.genai import types

import requests
import io

# ------------ –õ–û–ì–ò ------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("semantic-bot")
for noisy in ("httpx", "google_genai", "google_genai.models"):
    logging.getLogger(noisy).setLevel(logging.WARNING)

# ------------ –ö–û–ù–§–ò–ì ------------
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
if not TELEGRAM_BOT_TOKEN:
    raise RuntimeError("TELEGRAM_BOT_TOKEN –Ω–µ –∑–∞–¥–∞–Ω")

TEXT_MODEL_NAME = os.getenv("TEXT_MODEL_NAME", "gemini-2.5-flash")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "gemini-embedding-001")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "").strip()
if not GEMINI_API_KEY:
    raise RuntimeError("GEMINI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω")

# –ü—É—Ç–∏
DOC_FOLDER = os.getenv("DOC_FOLDER", "/tmp/documents")
INDEX_FILE = os.getenv("FAISS_INDEX", "/tmp/index.faiss")
TEXTS_FILE = os.getenv("TEXTS_FILE", "/tmp/texts.pkl")
MANIFEST_FILE = os.getenv("MANIFEST_FILE", "/tmp/manifest.json")
USAGE_FILE = os.getenv("USAGE_FILE", "/tmp/usage.json")
DOCMETA_FILE = os.getenv("DOCMETA_FILE", "/tmp/docmeta.json")  # –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

DAILY_FREE_LIMIT = int(os.getenv("DAILY_FREE_LIMIT", "10"))
ADMIN_USER_IDS = set(int(x.strip()) for x in os.getenv("ADMIN_USER_IDS", "").split(",") if x.strip().isdigit())

MAX_OUTPUT_TOKENS = int(os.getenv("MAX_OUTPUT_TOKENS", "2048"))
RETRIEVAL_K = int(os.getenv("RETRIEVAL_K", "6"))

# --- –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É–º–Ω–æ–≥–æ –¥–µ–ª–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏ ---
CHUNK_MAX_CHARS = int(os.getenv("CHUNK_MAX_CHARS", "2000"))
CHUNK_MIN_CHARS = int(os.getenv("CHUNK_MIN_CHARS", "400"))
SUBCHUNK_MAX_CHARS = int(os.getenv("SUBCHUNK_MAX_CHARS", "1600"))
TELEGRAM_MSG_LIMIT = 4096

# ------------ –¢–û–í–ê–†–ù–´–ï –ó–ù–ê–ö–ò (Google Sheets) ------------
TM_SHEET_ID = os.getenv("TM_SHEET_ID", "").strip()
TM_SHEET_NAME = os.getenv("TM_SHEET_NAME", "–õ–∏—Å—Ç1").strip()
TM_SHEET_GID = os.getenv("TM_SHEET_GID", "0").strip()
TM_ENABLE = os.getenv("TM_ENABLE", "1") == "1"
TM_SHEET_CSV_URL = os.getenv("TM_SHEET_CSV_URL", "").strip()
TM_DEBUG = os.getenv("TM_DEBUG", "0") == "1"

# ------------ –†–µ–∂–∏–º –∑–∞–ø—É—Å–∫–∞ ------------
RUN_MODE = os.getenv("RUN_MODE", "polling").strip().lower()  # "polling" | "webhook"
PUBLIC_BASE_URL = os.getenv("PUBLIC_BASE_URL", "").rstrip("/")

# ------------ –ö–ù–û–ü–ö–ò UI ------------
AI_LABEL = "ü§ñ AI-—á–∞—Ç"
DOCS_LABEL = "üìÑ –í–æ–ø—Ä–æ—Å—ã –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"
TM_LABEL = "üè∑Ô∏è –¢–æ–≤–∞—Ä–Ω—ã–µ –∑–Ω–∞–∫–∏"
MAIN_KB = ReplyKeyboardMarkup([[AI_LABEL, DOCS_LABEL, TM_LABEL]], resize_keyboard=True)

TM_LABELS = ['‚Ññ', '–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏', '–ù–æ–º–µ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏', '', '–û–ø–∏—Å–∞–Ω–∏–µ', '–°—Ç–∞—Ç—É—Å', '–°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è', '–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏', '', '–°—Å—ã–ª–∫–∞']

# –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –∫–∞—Ç–∞–ª–æ–≥–∏
def _ensure_dir(path: str) -> bool:
    try:
        Path(path).mkdir(parents=True, exist_ok=True)
        return True
    except PermissionError:
        return False

if not _ensure_dir(DOC_FOLDER):
    base = "/opt/render/project/src/data"
    if not _ensure_dir(base):
        base = "/tmp/data"
        _ensure_dir(base)
    logger.warning("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ %s. –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ %s", DOC_FOLDER, base)
    DOC_FOLDER   = os.path.join(base, "documents")
    INDEX_FILE   = os.path.join(base, "index.faiss")
    TEXTS_FILE   = os.path.join(base, "texts.pkl")
    MANIFEST_FILE= os.path.join(base, "manifest.json")
    _ensure_dir(DOC_FOLDER)

Path(Path(INDEX_FILE).parent).mkdir(parents=True, exist_ok=True)
Path(Path(TEXTS_FILE).parent).mkdir(parents=True, exist_ok=True)
Path(Path(MANIFEST_FILE).parent).mkdir(parents=True, exist_ok=True)
Path(Path(USAGE_FILE).parent).mkdir(parents=True, exist_ok=True)
Path(Path(DOCMETA_FILE).parent).mkdir(parents=True, exist_ok=True)

# ------------ –ö–õ–ò–ï–ù–¢ GEMINI ------------
client = genai.Client(api_key=GEMINI_API_KEY)

# ------------ –•–ï–õ–ü–ï–†–´ –õ–ò–ú–ò–¢–ê ------------
def _today_str() -> str:
    return time.strftime("%Y-%m-%d", time.localtime())

def _load_usage() -> dict:
    if os.path.exists(USAGE_FILE):
        try:
            with open(USAGE_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å USAGE_FILE: %s", e)
    return {}

def _save_usage(d: dict):
    try:
        with open(USAGE_FILE, "w", encoding="utf-8") as f:
            json.dump(d, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å USAGE_FILE: %s", e)

def is_admin(user_id: int) -> bool:
    return user_id in ADMIN_USER_IDS

def get_usage(user_id: int) -> int:
    data = _load_usage()
    today = _today_str()
    return int(data.get(str(user_id), {}).get(today, 0))

def inc_usage(user_id: int) -> int:
    data = _load_usage()
    today = _today_str()
    u = data.setdefault(str(user_id), {})
    u[today] = int(u.get(today, 0)) + 1
    _save_usage(data)
    return u[today]

# ------------ –§–ê–ô–õ–´/–ú–ê–ù–ò–§–ï–°–¢ ------------
def sha256_file(path: str) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()

def load_manifest() -> dict:
    if os.path.exists(MANIFEST_FILE):
        with open(MANIFEST_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_manifest(m: dict):
    with open(MANIFEST_FILE, "w", encoding="utf-8") as f:
        json.dump(m, f, ensure_ascii=False, indent=2)

# ------------ –ú–ï–¢–ê–î–ê–ù–ù–´–ï –î–û–ö–£–ú–ï–ù–¢–û–í ------------
def load_docmeta() -> dict:
    if os.path.exists(DOCMETA_FILE):
        try:
            with open(DOCMETA_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    return {}

def save_docmeta(meta: dict):
    with open(DOCMETA_FILE, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

def load_texts() -> List[str]:
    if os.path.exists(TEXTS_FILE):
        with open(TEXTS_FILE, "rb") as f:
            return pickle.load(f)
    return []

def save_texts(texts: List[str]):
    with open(TEXTS_FILE, "wb") as f:
        pickle.dump(texts, f)

def load_index() -> faiss.Index | None:
    if os.path.exists(INDEX_FILE):
        return faiss.read_index(INDEX_FILE)
    return None

def save_index(index: faiss.Index):
    faiss.write_index(index, INDEX_FILE)

# ------------ –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –¢–ï–ö–°–¢–ê ------------
def extract_text_from_pdf(file_path: str) -> str:
    try:
        doc = fitz.open(file_path)
        pages_text = []
        for page in doc:
            txt = page.get_text().strip()
            pages_text.append(txt)
        text = "\n".join(pages_text).strip()
        if text:
            return text
    except Exception as e:
        logger.warning("PyMuPDF –Ω–µ —Å–º–æ–≥ –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç (%s). –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ OCR.", repr(e))
    try:
        images = convert_from_path(file_path)
        ocr_texts = [pytesseract.image_to_string(img) for img in images]
        return "\n".join(ocr_texts).strip()
    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ OCR/convert_from_path: %s", repr(e))
        return ""

def extract_text_from_docx(file_path: str) -> str:
    try:
        doc = DocxDocument(file_path)
        return "\n".join([p.text for p in doc.paragraphs]).strip()
    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è DOCX: %s", repr(e))
        return ""

def extract_text_from_txt(file_path: str) -> str:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read().strip()
    except UnicodeDecodeError:
        with open(file_path, "r", encoding="cp1251", errors="ignore") as f:
            return f.read().strip()
    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è TXT: %s", repr(e))
        return ""

# ------------ –£–ú–ù–û–ï –î–ï–õ–ï–ù–ò–ï –ù–ê –ß–ê–ù–ö–ò ------------
CONTRACT_SECTIONS = [
    "–ü—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞","–ü—Ä–∞–≤–∞ –∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ —Å—Ç–æ—Ä–æ–Ω","–û–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ —Å—Ç–æ—Ä–æ–Ω","–ì–∞—Ä–∞–Ω—Ç–∏–∏ —Å—Ç–æ—Ä–æ–Ω","–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å —Å—Ç–æ—Ä–æ–Ω",
    "–°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –¥–æ–≥–æ–≤–æ—Ä–∞","–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —É—Å–ª–æ–≤–∏—è","–°—Ç–æ–∏–º–æ—Å—Ç—å —É—Å–ª—É–≥ –∏ –ø–æ—Ä—è–¥–æ–∫ –æ–ø–ª–∞—Ç—ã","–ü–æ—Ä—è–¥–æ–∫ –æ–ø–ª–∞—Ç—ã","–¢–µ—Ä–º–∏–Ω—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è",
    "–ü—Ä–æ—á–∏–µ —É—Å–ª–æ–≤–∏—è","–û–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞ –Ω–µ–ø—Ä–µ–æ–¥–æ–ª–∏–º–æ–π —Å–∏–ª—ã","–§–æ—Ä—Å-–º–∞–∂–æ—Ä","–ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å","–ü—Ä–∞–≤–æ –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
    "–ü–æ—Ä—è–¥–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏","–ó–∞–≤–µ—Ä–µ–Ω–∏—è –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞",
    "–ê–¥—Ä–µ—Å –∏ –±–∞–Ω–∫–æ–≤—Å–∫–∏–µ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã","–†–µ–∫–≤–∏–∑–∏—Ç—ã –∏ –ø–æ–¥–ø–∏—Å–∏ —Å—Ç–æ—Ä–æ–Ω","–ü–æ–¥–ø–∏—Å–∏ —Å—Ç–æ—Ä–æ–Ω"
]
POSITION_SECTIONS = [
    "–û–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è","–¶–µ–ª–∏ –∏ –∑–∞–¥–∞—á–∏","–ü—Ä–µ–¥–º–µ—Ç —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è","–¢–µ—Ä–º–∏–Ω—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è","–§—É–Ω–∫—Ü–∏–∏","–ü—Ä–∞–≤–∞ –∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏",
    "–ü—Ä–∞–≤–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏","–û–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏","–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å","–ü–æ—Ä—è–¥–æ–∫ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è","–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã",
    "–ü–æ—Ä—è–¥–æ–∫ –≤–Ω–µ—Å–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π","–ó–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è"
]
GENERIC_SECTIONS = [
    "–í–≤–µ–¥–µ–Ω–∏–µ","–û–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è","–¢–µ—Ä–º–∏–Ω—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è","–ü–æ—Ä—è–¥–æ–∫","–ü—Ä–∞–≤–∞ –∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏","–ü—Ä–∞–≤–∞","–û–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏","–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å",
    "–°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è","–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —É—Å–ª–æ–≤–∏—è","–ü–æ—Ä—è–¥–æ–∫ –æ–ø–ª–∞—Ç—ã","–ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å","–ü—Ä–æ—á–∏–µ —É—Å–ª–æ–≤–∏—è","–ó–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è","–ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è"
]

HEAD_NUM_RE = re.compile(r"^(?:—Ä–∞–∑–¥–µ–ª|–≥–ª–∞–≤–∞|section|chapter)\s+\d+[.:)]?$", re.IGNORECASE|re.MULTILINE)
HEAD_NUM2_RE = re.compile(r"^\d+(?:\.\d+)*[.)]?\s+\S+", re.MULTILINE)
HEAD_ROMAN_RE = re.compile(r"^(?:[IVXLCDM]+)[\.\)]\s+\S+", re.IGNORECASE|re.MULTILINE)

def guess_doc_type(text: str) -> str:
    head = text[:5000].lower()
    if "–¥–æ–≥–æ–≤–æ—Ä" in head:
        return "contract"
    if "–ø–æ–ª–æ–∂–µ–Ω–∏–µ" in head:
        return "position"
    return "generic"

def is_all_caps_cyr(line: str) -> bool:
    s = line.strip()
    if len(s) < 3 or len(s) > 120:
        return False
    letters = [ch for ch in s if ch.isalpha()]
    if not letters:
        return False
    upp = sum(1 for ch in letters if ch.upper() == ch and "–∞" <= ch.lower() <= "—è")
    return upp >= max(3, int(0.7 * len(letters)))

def find_headings(text: str, headings: List[str]) -> List[tuple[int,str]]:
    res = []
    for h in headings:
        for m in re.finditer(rf"(?im)^\s*{re.escape(h)}\s*$", text):
            res.append((m.start(), h))
    for m in HEAD_NUM_RE.finditer(text):
        res.append((m.start(), text[m.start():m.end()]))
    for m in HEAD_NUM2_RE.finditer(text):
        res.append((m.start(), text[m.start():m.end()].strip()))
    for m in HEAD_ROMAN_RE.finditer(text):
        res.append((m.start(), text[m.start():m.end()].strip()))
    for m in re.finditer(r"(?m)^(?P<line>.+)$", text):
        line = m.group("line")
        if is_all_caps_cyr(line):
            res.append((m.start(), line.strip()))
    uniq = {}
    for off, ttl in res:
        uniq[off] = ttl
    return sorted(uniq.items(), key=lambda x: x[0])

def split_by_sections(text: str, headings: List[str]) -> List[tuple[str,str]]:
    marks = find_headings(text, headings)
    if not marks:
        return [("", text.strip())]
    chunks = []
    for i, (start, title) in enumerate(marks):
        end = marks[i+1][0] if i+1 < len(marks) else len(text)
        chunk = text[start:end].strip()
        lines = chunk.splitlines()
        if lines:
            first_line = lines[0].strip()
            if len(first_line) <= 180:
                title = first_line
        chunks.append((title.strip(), chunk))
    return chunks

def split_long_chunk(title: str, body: str) -> List[str]:
    paras = [p.strip() for p in re.split(r"\n\s*\n", body) if p.strip()]
    out = []
    cur = title + "\n"
    for p in paras:
        add_len = 2 + len(p) if cur.strip() != title else len(p)
        if len(cur) + add_len > SUBCHUNK_MAX_CHARS and cur.strip():
            out.append(cur.strip())
            cur = title + "\n" + p
        else:
            if cur.strip() == title:
                cur = title + "\n" + p
            else:
                cur += "\n\n" + p
    if cur.strip():
        out.append(cur.strip())
    return out

def smart_split_text(text: str) -> List[str]:
    if not text or len(text.strip()) == 0:
        return []
    text = re.sub(r"\r\n?", "\n", text)

    doc_type = guess_doc_type(text)
    base_sections = CONTRACT_SECTIONS if doc_type=="contract" else POSITION_SECTIONS if doc_type=="position" else GENERIC_SECTIONS

    section_chunks = split_by_sections(text, base_sections)

    normalized: List[str] = []
    buf = ""
    for title, chunk in section_chunks:
        chunk = chunk.strip()
        if not chunk:
            continue
        if len(chunk) < CHUNK_MIN_CHARS and buf:
            buf += "\n\n" + chunk
            continue
        if buf:
            normalized.append(buf)
        buf = chunk
    if buf:
        normalized.append(buf)

    final_chunks: List[str] = []
    for ch in normalized:
        if len(ch) <= CHUNK_MAX_CHARS:
            final_chunks.append(ch)
        else:
            lines = ch.splitlines()
            title = lines[0].strip() if lines else "–†–∞–∑–¥–µ–ª"
            body = "\n".join(lines[1:]).strip() if len(lines) > 1 else ch
            parts = split_long_chunk(title, body)
            for p in parts:
                if len(p) > CHUNK_MAX_CHARS:
                    sents = re.split(r"(?<=[.!?‚Ä¶])\s+", p)
                    tmp = ""
                    for s in sents:
                        if len(tmp) + len(s) + 1 > CHUNK_MAX_CHARS:
                            if tmp.strip():
                                final_chunks.append(tmp.strip())
                            tmp = s
                        else:
                            tmp = (tmp + " " + s).strip()
                    if tmp.strip():
                        final_chunks.append(tmp.strip())
                else:
                    final_chunks.append(p.strip())

    if not final_chunks:
        sents = re.split(r"(?<=[.!?‚Ä¶])\s+", text)
        tmp = ""
        for s in sents:
            if len(tmp) + len(s) + 1 > CHUNK_MAX_CHARS:
                if tmp.strip():
                    final_chunks.append(tmp.strip())
                tmp = s
            else:
                tmp = (tmp + " " + s).strip()
        if tmp.strip():
            final_chunks.append(tmp.strip())

    return [c for c in final_chunks if c and c.strip()]

# ------------ –≠–ú–ë–ï–î–î–ò–ù–ì–ò/–ò–ù–î–ï–ö–° ------------
def get_embedding(text: str) -> np.ndarray:
    max_attempts = 5
    base = 1.0
    for attempt in range(1, max_attempts + 1):
        try:
            resp = client.models.embed_content(model=EMBEDDING_MODEL, contents=text)
            vec = resp.embeddings[0].values
            return np.array(vec, dtype="float32")
        except Exception as e:
            msg = repr(e)
            transient = any(code in msg for code in ["503", "UNAVAILABLE", "429", "502", "504"])
            if attempt < max_attempts and transient:
                sleep_s = base * (2 ** (attempt - 1)) + random.uniform(0, 0.15)
                logger.warning("–≠–º–±–µ–¥–¥–∏–Ω–≥ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (%s). –ü–æ–≤—Ç–æ—Ä #%d —á–µ—Ä–µ–∑ %.1f c", msg, attempt, sleep_s)
                time.sleep(sleep_s)
                continue
            logger.exception("–û—à–∏–±–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (–ø–æ—Å–ª–µ %d –ø–æ–ø—ã—Ç–æ–∫): %s", attempt, msg)
            raise

def ensure_index(dim: int) -> faiss.Index:
    index = load_index()
    if index is None:
        index = faiss.IndexFlatL2(dim)
    return index

def index_file(file_path: str) -> Tuple[int, int]:
    ext = file_path.lower().split(".")[-1]
    if ext == "pdf":
        text = extract_text_from_pdf(file_path)
    elif ext == "docx":
        text = extract_text_from_docx(file_path)
    elif ext == "txt":
        text = extract_text_from_txt(file_path)
    else:
        raise ValueError("–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞.")
    if not text:
        logger.warning("–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: %s", os.path.basename(file_path))
        return (0, 0)

    chunks = smart_split_text(text)
    if not chunks:
        return (0, 0)

    texts = load_texts()
    first_vec = None
    for ch in chunks:
        try:
            first_vec = get_embedding(ch)
            break
        except Exception as e:
            logger.warning("–ü—Ä–æ–ø—É—â–µ–Ω —á–∞–Ω–∫ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: %s", repr(e))
            continue
    if first_vec is None:
        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.")
    index = ensure_index(len(first_vec))
    new_embeddings, new_texts = [], []
    for ch in chunks:
        try:
            emb = get_embedding(ch)
            new_embeddings.append(emb)
            new_texts.append(ch)
            time.sleep(0.02)
        except Exception as e:
            logger.warning("–ü—Ä–æ–ø—É—â–µ–Ω —á–∞–Ω–∫ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: %s", repr(e))
            continue
    if not new_embeddings:
        existing = load_index()
        return (0, existing.ntotal if existing else 0)

    # –ò–Ω–¥–µ–∫—Å—ã —á–∞–Ω–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã
    base_ntotal = 0
    existing_index = load_index()
    if existing_index is not None:
        base_ntotal = getattr(existing_index, "ntotal", 0)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å –∏ —Ç–µ–∫—Å—Ç—ã
    mat = np.vstack(new_embeddings).astype("float32")
    index.add(mat)
    save_index(index)

    texts.extend(new_texts)
    save_texts(texts)

    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤–ª–∞–¥–µ–ª—å—Ü–∞ —á–∞–Ω–∫–æ–≤
    file_hash = sha256_file(file_path)
    meta = load_docmeta()
    added_ids = list(range(base_ntotal, base_ntotal + len(new_embeddings)))
    rec = meta.get(file_hash, {"fname": os.path.basename(file_path), "time": int(time.time()), "chunks": []})
    rec["fname"] = os.path.basename(file_path)
    rec["time"] = int(time.time())
    rec["chunks"] = sorted(set((rec.get("chunks") or []) + added_ids))
    meta[file_hash] = rec
    save_docmeta(meta)

    total = index.ntotal if hasattr(index, "ntotal") else len(texts)
    return (len(new_texts), total)

def retrieve_chunks(query: str, k: int = RETRIEVAL_K) -> List[str]:
    if not (os.path.exists(INDEX_FILE) and os.path.exists(TEXTS_FILE)):
        return []
    q_emb = get_embedding(query)
    index = load_index()
    texts = load_texts()
    if index is None or len(texts) == 0:
        return []
    D, I = index.search(np.array([q_emb], dtype="float32"), k=min(k, len(texts)))
    ids = [i for i in I[0] if 0 <= i < len(texts)]
    return [texts[i] for i in ids]

def rebuild_faiss_from_texts(texts: List[str]) -> int:
    """–ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë—Ç FAISS-–∏–Ω–¥–µ–∫—Å –∏–∑ —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤."""
    if not texts:
        if os.path.exists(INDEX_FILE):
            os.remove(INDEX_FILE)
        save_texts([])
        return 0

    # –û–ø—Ä–µ–¥–µ–ª–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
    dim_vec = None
    for t in texts:
        try:
            dim_vec = get_embedding(t)
            break
        except Exception:
            continue
    if dim_vec is None:
        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞.")

    index = faiss.IndexFlatL2(len(dim_vec))
    embs = []
    for t in texts:
        try:
            embs.append(get_embedding(t))
            time.sleep(0.01)
        except Exception as e:
            logger.warning("rebuild: –ø—Ä–æ–ø—É—â–µ–Ω —á–∞–Ω–∫ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: %r", e)
            continue
    if not embs:
        raise RuntimeError("rebuild: –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ –≤–∞–ª–∏–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞.")
    mat = np.vstack(embs).astype("float32")
    index.add(mat)
    save_index(index)
    save_texts(texts)
    return index.ntotal

def delete_document_from_training(file_hash: str, also_remove_file: bool = False) -> tuple[bool, str]:
    """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ texts.pkl –∏ –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ—Ç FAISS.
       also_remove_file=True ‚Äî –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É–¥–∞–ª—è–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏–π —Ñ–∞–π–ª –∏–∑ DOC_FOLDER (–µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç).
       –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (ok, message)."""
    meta = load_docmeta()
    if file_hash not in meta:
        return False, "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ø–æ —ç—Ç–æ–º—É —Ñ–∞–π–ª—É (–≤–æ–∑–º–æ–∂–Ω–æ, –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–ª–æ—Å—å —Å—Ç–∞—Ä–æ–π –≤–µ—Ä—Å–∏–µ–π –±–µ–∑ —É—á—ë—Ç–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤). " \
                      "–í–æ—Å–ø–æ–ª—å–∑—É–π—Ç–µ—Å—å /admin_rebuild –¥–ª—è –ø–æ–ª–Ω–æ–π –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏."

    # –ó–∞–ø–æ–º–Ω–∏–º –∏–º—è –∑–∞—Ä–∞–Ω–µ–µ (–¥–æ —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ meta)
    fname_hint = meta[file_hash].get("fname", "")

    # –ì–æ—Ç–æ–≤–∏–º –Ω–æ–≤–æ–µ —Ç–µ–ª–æ —Ç–µ–∫—Å—Ç–æ–≤ –±–µ–∑ —É–¥–∞–ª—è–µ–º—ã—Ö —á–∞–Ω–∫–æ–≤
    texts = load_texts()
    drop_ids = set(meta[file_hash].get("chunks") or [])
    if not texts:
        return False, "texts.pkl –ø—É—Å—Ç ‚Äî –Ω–µ—á–µ–≥–æ —É–¥–∞–ª—è—Ç—å."
    keep_texts = [t for i, t in enumerate(texts) if i not in drop_ids]

    # –ü–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º –∏–Ω–¥–µ–∫—Å
    try:
        total = rebuild_faiss_from_texts(keep_texts)
    except Exception as e:
        return False, f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e!r}"

    # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: —É–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å—å, —É –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —É–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ä—ã–µ –∏–Ω–¥–µ–∫—Å—ã chunks
    meta.pop(file_hash, None)
    for k in list(meta.keys()):
        if "chunks" in meta[k]:
            meta[k].pop("chunks", None)
    save_docmeta(meta)

    # –û–±–Ω–æ–≤–ª—è–µ–º manifest (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —É–±–∏—Ä–∞–µ–º —Å–ª–µ–¥)
    man = load_manifest()
    hs = man.get("hashes", {})
    if file_hash in hs:
        hs.pop(file_hash, None)
        man["hashes"] = hs
        with open(MANIFEST_FILE, "w", encoding="utf-8") as f:
            json.dump(man, f, ensure_ascii=False, indent=2)

    removed_file_msg = ""
    if also_remove_file and fname_hint:
        try:
            for name in os.listdir(DOC_FOLDER):
                if name == fname_hint:
                    os.unlink(os.path.join(DOC_FOLDER, name))
                    removed_file_msg = f" –§–∞–π–ª {fname_hint} —É–¥–∞–ª—ë–Ω."
                    break
        except Exception as e:
            removed_file_msg = f" –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª: {e!r}"

    return True, f"–î–æ–∫—É–º–µ–Ω—Ç —É–¥–∞–ª—ë–Ω –∏–∑ –æ–±—É—á–µ–Ω–∏—è. –¢–µ–∫—É—â–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ: {total}.{removed_file_msg}"

def full_reindex_all_documents() -> tuple[int, list[str]]:
    """–ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –ø–æ –≤—Å–µ–º —Ñ–∞–π–ª–∞–º –≤ DOC_FOLDER. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (total_vectors, errors)."""
    # –°–±—Ä–æ—Å —Ç–µ–∫—É—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞/—Ç–µ–∫—Å—Ç–æ–≤/–º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    if os.path.exists(INDEX_FILE):
        os.remove(INDEX_FILE)
    save_texts([])
    save_docmeta({})

    errors = []
    total_added = 0
    for name in sorted(os.listdir(DOC_FOLDER)):
        path = os.path.join(DOC_FOLDER, name)
        if not os.path.isfile(path):
            continue
        ext = os.path.splitext(name)[1].lower()
        if ext not in (".pdf", ".docx", ".txt"):
            continue
        try:
            added, _ = index_file(path)
            total_added += added
            time.sleep(0.05)
        except Exception as e:
            errors.append(f"{name}: {e!r}")
    idx = load_index()
    total_vectors = getattr(idx, "ntotal", 0) if idx else 0
    return total_vectors, errors

# ------------ –ì–ï–ù–ï–†–ê–¶–ò–Ø ------------
def generate_answer_with_gemini(user_query: str, retrieved_chunks: List[str]) -> str:
    context = "\n\n".join(retrieved_chunks[:RETRIEVAL_K]) if retrieved_chunks else "(–∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω)"
    prompt = (
        "–í—ã ‚Äî —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –ø–æ–º–æ—â–Ω–∏–∫. –î–∞–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π, –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–π –æ—Ç–≤–µ—Ç.\n\n"
        "–ò–°–ü–û–õ–¨–ó–£–ô –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç—ã –∏–∑ –ö–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–∏–∂–µ. –ï—Å–ª–∏ —á–µ–≥–æ-—Ç–æ –Ω–µ—Ç –≤ –ö–æ–Ω—Ç–µ–∫—Å—Ç–µ ‚Äî –ø—Ä—è–º–æ —Å–∫–∞–∂–∏, –Ω–µ –≤—ã–¥—É–º—ã–≤–∞–π.\n\n"
        "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞:\n"
        "1) –ö—Ä–∞—Ç–∫–∏–π –∏—Ç–æ–≥ –≤ 2‚Äì4 —Å—Ç—Ä–æ–∫–∞—Ö.\n"
        "2) –ü–æ–¥—Ä–æ–±–Ω—ã–π —Ä–∞–∑–±–æ—Ä –ø–æ –ø—É–Ω–∫—Ç–∞–º, —É–∫–∞–∑—ã–≤–∞—è –ò–ú–Ø –¥–æ–∫—É–º–µ–Ω—Ç–∞, –≥–¥–µ —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –ø–æ–ª–æ–∂–µ–Ω–∏—è.\n"
        "3) –¶–∏—Ç–∞—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞.\n"
        "4) –ß—ë—Ç–∫–∏–µ —à–∞–≥–∏.\n\n"
        f"–ö–û–ù–¢–ï–ö–°–¢:\n{context}\n\n"
        f"–ó–ê–ü–†–û–°:\n{user_query}"
    )
    try:
        resp = client.models.generate_content(
            model=TEXT_MODEL_NAME,
            contents=[prompt],
            config=types.GenerateContentConfig(
                temperature=0.2,
                max_output_tokens=MAX_OUTPUT_TOKENS,
            ),
        )
        if getattr(resp, "text", None):
            return resp.text.strip()
        pf = getattr(resp, "prompt_feedback", None)
        if pf and getattr(pf, "block_reason", None):
            return f"‚ö†Ô∏è –ó–∞–ø—Ä–æ—Å –æ—Ç–∫–ª–æ–Ω—ë–Ω –º–æ–¥–µ—Ä–∞—Ü–∏–µ–π: {pf.block_reason}."
        cands = getattr(resp, "candidates", []) or []
        for c in cands:
            if getattr(c, "content", None) and getattr(c.content, "parts", None):
                parts_text = "".join(getattr(p, "text", "") for p in c.content.parts)
                if parts_text.strip():
                    return parts_text.strip()
        return "‚ö†Ô∏è –û—Ç–≤–µ—Ç –ø—É—Å—Ç."
    except Exception as e:
        msg = repr(e)
        if any(x in msg for x in ["429", "503", "502", "504", "UNAVAILABLE", "ResourceExhausted"]):
            return "‚ö†Ô∏è –ü–µ—Ä–µ–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–∑–∂–µ."
        if "401" in msg or "403" in msg or "PermissionDenied" in msg:
            return "‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π –∫–ª—é—á –∏–ª–∏ –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞."
        return f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {msg}"

def generate_direct_ai_answer(user_query: str) -> str:
    system = (
        "–¢—ã ‚Äî –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π —á—ë—Ç–∫–æ, –ø–æ –¥–µ–ª—É. "
        "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∏ —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –¥–∞–≤–∞–π –æ–±—â–∏–π —Å–æ–≤–µ—Ç –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–π –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ —é—Ä–∏—Å—Ç–æ–º."
    )
    prompt = f"–°–ò–°–¢–ï–ú–ê:\n{system}\n\n–ó–ê–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:\n{user_query}"
    try:
        resp = client.models.generate_content(
            model=TEXT_MODEL_NAME,
            contents=[prompt],
            config=types.GenerateContentConfig(
                temperature=0.3,
                max_output_tokens=MAX_OUTPUT_TOKENS,
            ),
        )
        if getattr(resp, "text", None):
            return resp.text.strip()
        return "‚ö†Ô∏è –û—Ç–≤–µ—Ç –ø—É—Å—Ç."
    except Exception as e:
        msg = repr(e)
        if any(x in msg for x in ["429", "503", "502", "504", "UNAVAILABLE", "ResourceExhausted"]):
            return "‚ö†Ô∏è –ü–µ—Ä–µ–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–∑–∂–µ."
        if "401" in msg or "403" in msg or "PermissionDenied" in msg:
            return "‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π –∫–ª—é—á –∏–ª–∏ –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞."
        return f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {msg}"

def _split_for_telegram(text: str, max_len: int = TELEGRAM_MSG_LIMIT - 200) -> list[str]:
    parts, buf = [], []
    cur = 0
    paragraphs = text.replace("\r\n", "\n").split("\n\n")
    for p in paragraphs:
        p = p.strip()
        if not p:
            candidate = "\n\n".join(buf).strip()
            if candidate:
                parts.append(candidate)
                buf, cur = [], 0
            continue
        delta = len(p) + (2 if cur > 0 else 0)
        if cur + delta <= max_len:
            buf.append(p); cur += delta
        else:
            candidate = "\n\n".join(buf).strip()
            if candidate:
                parts.append(candidate)
            buf, cur = [], 0
            while len(p) > max_len:
                parts.append(p[:max_len]); p = p[max_len:]
            if p:
                buf = [p]; cur = len(p)
    candidate = "\n\n".join(buf).strip()
    if candidate:
        parts.append(candidate)
    return parts

async def send_long(update: Update, text: str):
    for chunk in _split_for_telegram(text):
        await update.message.reply_text(chunk, disable_web_page_preview=True)

# ------------ TM: URL/IMAGE HELPERS ------------
def _html_escape(text: str) -> str:
    return (text.replace("&", "&amp;")
                .replace("<", "&lt;")
                .replace(">", "&gt;")
                .replace('"', "&quot;")
                .replace("'", "&#039;"))

URL_RE = re.compile(r'(https?://[^\s<>")]+)', re.IGNORECASE)

def _extract_urls(cell: str) -> list[str]:
    """–î–æ—Å—Ç–∞—ë–º –í–°–ï URL –∏–∑ —è—á–µ–π–∫–∏ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Å—ã–ª–æ–∫)."""
    txt = str(cell or "")
    tokens = re.split(r'[,\s]+', txt.strip())
    urls = []
    for t in tokens:
        if t.startswith("http://") or t.startswith("https://"):
            urls.append(t)
        else:
            urls.extend(URL_RE.findall(t))
    seen, out = set(), []
    for u in urls:
        if u not in seen:
            seen.add(u); out.append(u)
    return out

def _normalize_image_url(url: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Google Drive —Å—Å—ã–ª–æ–∫ -> –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∞–π–ª."""
    u = url.strip()
    m = re.search(r'drive\.google\.com/file/d/([^/]+)/', u)
    if m:
        return f"https://drive.google.com/uc?export=download&id={m.group(1)}"
    m = re.search(r'(?:[?&]id=)([A-Za-z0-9_-]+)', u)
    if m and "drive.google.com" in u:
        return f"https://drive.google.com/uc?export=download&id={m.group(1)}"
    return u

def _is_probable_image_url(url: str) -> bool:
    u = url.lower()
    if any(u.endswith(ext) for ext in (".png", ".jpg", ".jpeg", ".gif", ".webp")):
        return True
    if "googleusercontent.com" in u or "uc?export=download" in u or "=download" in u:
        return True
    return False

def _format_date(value: str) -> str:
    from datetime import datetime
    for fmt in ("%d.%m.%Y", "%d/%m/%Y", "%Y-%m-%d", "%d-%m-%Y", "%m/%d/%Y"):
        try:
            return datetime.strptime(value.strip(), fmt).strftime("%d/%m/%Y")
        except Exception:
            pass
    return value.strip()

def tm_format_row(row: list[str], labels: list[str] = TM_LABELS) -> tuple[str, list[str]]:
    formatted_lines = []
    image_urls: list[str] = []

    for idx, val in enumerate(row):
        label = labels[idx] if idx < len(labels) else ""
        if idx == 3:
            continue

        cell = str(val or "").strip()
        if not cell:
            continue

        # 1) –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º –≤—Å–µ URL –∏–∑ —è—á–µ–π–∫–∏
        urls = _extract_urls(cell)
        for u in urls:
            nu = _normalize_image_url(u)
            if _is_probable_image_url(nu):
                image_urls.append(nu)

        # 2) –µ—Å–ª–∏ —è—á–µ–π–∫–∞ ‚Äî —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏, –Ω–µ –¥—É–±–ª–∏—Ä—É–µ–º –∏—Ö –≤ —Ç–µ–∫—Å—Ç–µ
        only_links = urls and (cell == " ".join(urls) or cell == ",".join(urls))
        if only_links:
            continue

        if re.match(r"^\d{1,4}[-./]\d{1,2}[-./]\d{1,4}$", cell):
            cell = _format_date(cell)

        if label:
            formatted_lines.append(f"<b>{_html_escape(label)}:</b> {_html_escape(cell)}")
        else:
            formatted_lines.append(_html_escape(cell))

    # –¥–µ–¥—É–ø –∫–∞—Ä—Ç–∏–Ω–æ–∫
    seen, uniq_images = set(), []
    for u in image_urls:
        if u not in seen:
            seen.add(u); uniq_images.append(u)

    text = "\n".join(formatted_lines).strip()
    return text, uniq_images

async def _tm_send_image_safely(chat_id: int, url: str, context: ContextTypes.DEFAULT_TYPE) -> bool:
    """–°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º URL –Ω–∞–ø—Ä—è–º—É—é; –µ—Å–ª–∏ –Ω–µ –≤—ã—à–ª–æ ‚Äî —Å–∫–∞—á–∏–≤–∞–µ–º –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –±–∞–π—Ç—ã."""
    try:
        await context.bot.send_photo(chat_id, photo=url)
        return True
    except Exception as e:
        logger.warning("TM: send_photo by URL failed for %s: %r", url, e)

    try:
        r = requests.get(url, timeout=25)
        ct = (r.headers.get("Content-Type") or "").lower()
        if r.status_code == 200 and (r.content and ("image/" in ct or len(r.content) > 0)):
            buf = io.BytesIO(r.content)
            filename = "image"
            ul = url.lower()
            if ".png" in ul: filename += ".png"
            elif ".jpg" in ul or ".jpeg" in ul: filename += ".jpg"
            elif ".webp" in ul: filename += ".webp"
            elif ".gif" in ul: filename += ".gif"
            else:
                if "image/png" in ct: filename += ".png"
                elif "image/jpeg" in ct: filename += ".jpg"
                elif "image/webp" in ct: filename += ".webp"
                elif "image/gif" in ct: filename += ".gif"
                else: filename += ".bin"
            await context.bot.send_photo(chat_id, photo=InputFile(buf, filename=filename))
            return True
    except Exception as e:
        logger.warning("TM: fallback download failed for %s: %r", url, e)

    return False

# ------------ TM –∑–∞–≥—Ä—É–∑–∫–∞ CSV ------------
async def _tm_fetch_rows_csv(sheet_id: str, gid: str, sheet_name: str, override_url: str = "") -> list[list[str]]:
    import csv, io as _io
    urls = []
    if override_url:
        urls.append(override_url)
    if sheet_id:
        urls.append(f"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&gid={gid}")
        urls.append(f"https://docs.google.com/spreadsheets/d/{sheet_id}/pub?gid={gid}&single=true&output=csv")
        if sheet_name:
            from urllib.parse import quote
            urls.append(f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={quote(sheet_name)}")
    last_err = None
    for url in urls:
        try:
            resp = await asyncio.to_thread(requests.get, url, timeout=30)
            status = resp.status_code
            ctype = resp.headers.get("Content-Type", "")
            content = resp.content.decode("utf-8", errors="replace")
            if status == 200 and ("," in content or ";" in content or "\n" in content):
                reader = csv.reader(_io.StringIO(content))
                rows = [row for row in reader]
                if rows and any(any(cell.strip() for cell in row) for row in rows):
                    return rows
            last_err = f"Bad content from {url} (status={status}, type={ctype})"
        except Exception as e:
            last_err = f"{type(e).__name__}: {e} at {url}"
            continue
    raise RuntimeError(last_err or "CSV fetch failed")

async def tm_load_data() -> list[list[str]]:
    if not TM_ENABLE:
        return []
    try:
        return await _tm_fetch_rows_csv(TM_SHEET_ID, TM_SHEET_GID, TM_SHEET_NAME, TM_SHEET_CSV_URL)
    except Exception as e:
        logger.error("TM: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ª–∏—Å—Ç–∞: %s", repr(e))
        if TM_DEBUG:
            raise
        return []

def _row_matches_registered(row: list[str]) -> bool:
    col = (row[5] if len(row) > 5 else "") or ""
    return "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è" in col.lower()

def _row_matches_expertise(row: list[str]) -> bool:
    col = (row[5] if len(row) > 5 else "") or ""
    return "—ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞" in col.lower()

def _row_matches_keywords(row: list[str], keywords: list[str]) -> bool:
    low = [str(c or "").lower() for c in row]
    for kw in keywords:
        kw = kw.strip().lower()
        if kw and any(kw in cell for cell in low):
            return True
    return False

async def tm_process_search(chat_id: int, condition_cb, context: ContextTypes.DEFAULT_TYPE):
    try:
        data = await tm_load_data()
    except Exception as e:
        msg = "–î–∞–Ω–Ω—ã–µ –ª–∏—Å—Ç–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –∏–ª–∏ –ø—É—Å—Ç—ã."
        if TM_DEBUG:
            msg += f"\n\n–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: {e!r}\n–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏—é —Ç–∞–±–ª–∏—Ü—ã (File‚ÜíPublish to web), –≤–µ—Ä–Ω—ã–π GID –ª–∏—Å—Ç–∞ –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CSV-—ç–∫—Å–ø–æ—Ä—Ç–∞."
            msg += f"\n–¢–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: SHEET_ID={TM_SHEET_ID}, GID={TM_SHEET_GID}, SHEET_NAME={TM_SHEET_NAME}"
        await context.bot.send_message(chat_id, msg)
        return

    if not data or not any(data):
        note = "–î–∞–Ω–Ω—ã–µ –ª–∏—Å—Ç–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –∏–ª–∏ –ø—É—Å—Ç—ã."
        if TM_DEBUG:
            note += f"\n–ü—Ä–æ–≤–µ—Ä—å—Ç–µ: Publish to web –≤–∫–ª—é—á—ë–Ω, –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π GID, –ª–∏—Å—Ç –Ω–µ –ø—É—Å—Ç–æ–π."
        await context.bot.send_message(chat_id, note)
        return

    rows = data[1:] if len(data) > 1 else []
    matched_idx = [i for i, r in enumerate(rows, start=2) if condition_cb(r)]
    if not matched_idx:
        await context.bot.send_message(chat_id, "–î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return

    for i in matched_idx:
        row = data[i-1]
        text, images = tm_format_row(row)
        if text:
            await context.bot.send_message(chat_id, text, parse_mode="HTML", disable_web_page_preview=False)
        for url in images:
            ok = await _tm_send_image_safely(chat_id, url, context)
            if not ok:
                logger.warning("TM: failed to send image after fallback: %s", url)

# ------------ TELEGRAM HANDLERS ------------
TM_MODE = "tm"

def _admin_only(update: Update) -> bool:
    return is_admin(update.effective_user.id)

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    context.user_data["mode"] = "docs"
    usage_left = "‚àû" if is_admin(update.effective_user.id) else max(
        0, DAILY_FREE_LIMIT - get_usage(update.effective_user.id)
    )
    msg = (
        "–ü—Ä–∏–≤–µ—Ç!\n\n"
        "1) –ü—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª (.pdf, .docx –∏–ª–∏ .txt) –∏ –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é (—Ä–µ–∂–∏–º \"–í–æ–ø—Ä–æ—Å—ã –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º\").\n"
        "2) –ù–∞–∂–º–∏—Ç–µ \"ü§ñ AI-—á–∞—Ç\" –¥–ª—è –¥–∏–∞–ª–æ–≥–∞ –±–µ–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n"
        "3) –ù–∞–∂–º–∏—Ç–µ \"üè∑Ô∏è –¢–æ–≤–∞—Ä–Ω—ã–µ –∑–Ω–∞–∫–∏\" –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ Google Sheets.\n\n"
        f"–°–µ–≥–æ–¥–Ω—è—à–Ω–∏–π –ª–∏–º–∏—Ç AI-—á–∞—Ç: {usage_left} —Å–æ–æ–±—â–µ–Ω–∏–π."
    )
    await update.message.reply_text(msg, reply_markup=MAIN_KB)

async def ai_mode(update: Update, context: ContextTypes.DEFAULT_TYPE):
    context.user_data["mode"] = "ai"
    usage_left = "‚àû" if is_admin(update.effective_user.id) else max(0, DAILY_FREE_LIMIT - get_usage(update.effective_user.id))
    await update.message.reply_text(
        f"–†–µ–∂–∏–º: AI-—á–∞—Ç. –°–ø—Ä–æ—Å–∏—Ç–µ —á—Ç–æ —É–≥–æ–¥–Ω–æ. –î–æ—Å—Ç—É–ø–Ω–æ —Å–µ–≥–æ–¥–Ω—è: {usage_left}.", reply_markup=MAIN_KB
    )

async def docs_mode(update: Update, context: ContextTypes.DEFAULT_TYPE):
    context.user_data["mode"] = "docs"
    await update.message.reply_text(
        "–†–µ–∂–∏–º: –≤–æ–ø—Ä–æ—Å—ã –ø–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º. –ü—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª –∏ –∑–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å.", reply_markup=MAIN_KB
    )

async def tm_mode(update: Update, context: ContextTypes.DEFAULT_TYPE):
    context.user_data["mode"] = TM_MODE
    intro = (
        "–†–µ–∂–∏–º: üè∑Ô∏è –¢–æ–≤–∞—Ä–Ω—ã–µ –∑–Ω–∞–∫–∏.\n\n"
        "–û—Ç–ø—Ä–∞–≤—å—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ/–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ‚Äî –Ω–∞–π–¥—É —Å—Ç—Ä–æ–∫–∏ –≤ Google Sheets –∏ –ø—Ä–∏—à–ª—é –∫–∞—Ä—Ç–æ—á–∫–∏.\n"
        "–ö–æ–º–∞–Ω–¥—ã:\n"
        "‚Ä¢ /tm_reg ‚Äî –∑–∞–ø–∏—Å–∏, –≥–¥–µ —Å—Ç–∞—Ç—É—Å —Å–æ–¥–µ—Ä–∂–∏—Ç ¬´—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è¬ª\n"
        "‚Ä¢ /tm_exp ‚Äî –∑–∞–ø–∏—Å–∏, –≥–¥–µ —Å—Ç–∞—Ç—É—Å —Å–æ–¥–µ—Ä–∂–∏—Ç ¬´—ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞¬ª"
    )
    await update.message.reply_text(intro, reply_markup=MAIN_KB)

async def tm_cmd_reg(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await tm_process_search(update.effective_chat.id, _row_matches_registered, context)

async def tm_cmd_exp(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await tm_process_search(update.effective_chat.id, _row_matches_expertise, context)

async def tm_handle_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_text = (update.message.text or "").strip()
    kws = re.split(r"\s+", user_text)
    await tm_process_search(update.effective_chat.id, lambda row: _row_matches_keywords(row, kws), context)

async def handle_file(update: Update, context: ContextTypes.DEFAULT_TYPE):
    document: Document = update.message.document
    raw = document.file_name or "file"
    fname = Path(raw).name
    base, ext = os.path.splitext(fname)
    ext = ext.lower().lstrip(".")
    if ext not in ("pdf", "docx", "txt"):
        await update.message.reply_text("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ PDF, DOCX, TXT.")
        return
    os.makedirs(DOC_FOLDER, exist_ok=True)
    ts = int(time.time())
    file_path = os.path.join(DOC_FOLDER, f"{base}_{ts}.{ext}")
    new_file = await context.bot.get_file(document.file_id)
    await new_file.download_to_drive(file_path)

    manifest = load_manifest()
    file_hash = sha256_file(file_path)
    hashes = manifest.get("hashes", {})
    if file_hash in hashes:
        await update.message.reply_text("–≠—Ç–æ—Ç —Ñ–∞–π–ª —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω —Ä–∞–Ω–µ–µ. –ú–æ–∂–µ—Ç–µ –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã.")
        return

    await update.message.reply_text("–§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–∞—á–∞–ª–∞—Å—å...")
    def _index_job():
        try:
            added, total = index_file(file_path)
            return (True, added, total, None)
        except Exception as e:
            return (False, 0, 0, repr(e))
    ok, added, total, err = await asyncio.to_thread(_index_job)
    if ok:
        if added == 0:
            await update.message.reply_text(
                "–§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω, –Ω–æ —Ç–µ–∫—Å—Ç –Ω–µ –∏–∑–≤–ª–µ—á—ë–Ω. –í–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ —Å–∫–∞–Ω –±–µ–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–ª–æ—è.\n"
                "–ü—Ä–∏—à–ª–∏—Ç–µ DOCX/TXT –∏–ª–∏ PDF —Å —Ç–µ–∫—Å—Ç–æ–º, –ª–∏–±–æ –≤–∫–ª—é—á–∏—Ç–µ OCR (tesseract+poppler/Docker)."
            )
            return
        manifest.setdefault("hashes", {})[file_hash] = {"fname": os.path.basename(file_path), "time": int(time.time())}
        with open(MANIFEST_FILE, "w", encoding="utf-8") as f:
            json.dump(manifest, f, ensure_ascii=False, indent=2)
        await update.message.reply_text(
            f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –î–æ–±–∞–≤–ª–µ–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {added}. –í—Å–µ–≥–æ: {total}. –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã."
        )
    else:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {err}")

async def handle_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_query = (update.message.text or "").strip()
    mode = context.user_data.get("mode", "docs")

    if user_query == AI_LABEL:
        await ai_mode(update, context); return
    if user_query == DOCS_LABEL:
        await docs_mode(update, context); return
    if user_query == TM_LABEL:
        await tm_mode(update, context); return

    if mode == "ai":
        uid = update.effective_user.id
        if not is_admin(uid):
            used = get_usage(uid)
            if used >= DAILY_FREE_LIMIT:
                await update.message.reply_text(
                    "–î–æ—Å—Ç–∏–≥–Ω—É—Ç –¥–Ω–µ–≤–Ω–æ–π –ª–∏–º–∏—Ç –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –æ–±—Ä–∞—â–µ–Ω–∏–π –∫ –ò–ò (10). –í–æ–∑–≤—Ä–∞—â–∞–π—Ç–µ—Å—å –∑–∞–≤—Ç—Ä–∞ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.",
                    reply_markup=MAIN_KB,
                )
                return
        def _ai_job():
            try:
                return generate_direct_ai_answer(user_query)
            except Exception as e:
                return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {repr(e)}"
        answer = await asyncio.to_thread(_ai_job)
        if answer and not is_admin(uid):
            inc_usage(uid)
        await send_long(update, answer)
        if not is_admin(uid):
            left = max(0, DAILY_FREE_LIMIT - get_usage(uid))
            await update.message.reply_text(f"–û—Å—Ç–∞—Ç–æ–∫ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è: {left}.")
        return

    if mode == "tm":
        low = user_query.lower()
        if low.startswith("/tm_reg"):
            await tm_cmd_reg(update, context); return
        if low.startswith("/tm_exp"):
            await tm_cmd_exp(update, context); return
        await tm_handle_text(update, context)
        return

    if not (os.path.exists(INDEX_FILE) and os.path.exists(TEXTS_FILE)):
        await update.message.reply_text("–ù–µ—Ç –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª.")
        return

    def _answer_job():
        try:
            chunks = retrieve_chunks(user_query, k=RETRIEVAL_K)
            return generate_answer_with_gemini(user_query, chunks)
        except Exception as e:
            return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {repr(e)}"

    answer = await asyncio.to_thread(_answer_job)
    if not answer:
        await update.message.reply_text("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç.")
    else:
        await send_long(update, answer)

async def debug(update: Update, context: ContextTypes.DEFAULT_TYPE):
    def stat():
        def fs(path):
            exists = os.path.exists(path)
            size = Path(path).stat().st_size if exists and os.path.isfile(path) else 0
            return f"{path}: {'OK' if exists else '‚Äî'} (size={size})"
        lines = [
            fs(DOC_FOLDER),
            fs(INDEX_FILE),
            fs(TEXTS_FILE),
            fs(MANIFEST_FILE),
            fs(USAGE_FILE),
            fs(DOCMETA_FILE),
            f"ADMIN_USER_IDS={sorted(list(ADMIN_USER_IDS))}",
            f"DAILY_FREE_LIMIT={DAILY_FREE_LIMIT}",
        ]
        try:
            texts = load_texts()
            lines.append(f"texts count={len(texts) if isinstance(texts, list) else 0}")
        except Exception as e:
            lines.append(f"texts load error: {e!r}")
        try:
            idx = load_index()
            lines.append(f"faiss ntotal={getattr(idx, 'ntotal', 0) if idx else 0}")
        except Exception as e:
            lines.append(f"faiss load error: {e!r}")
        try:
            data = _load_usage()
            today = _today_str()
            today_sum = sum(int(v.get(today, 0)) for v in data.values())
            lines.append(f"AI-—á–∞—Ç –æ–±—Ä–∞—â–µ–Ω–∏–π —Å–µ–≥–æ–¥–Ω—è: {today_sum}")
        except Exception as e:
            lines.append(f"usage load error: {e!r}")
        return "\n".join(lines)
    out = await asyncio.to_thread(stat)
    await update.message.reply_text("–°–æ—Å—Ç–æ—è–Ω–∏–µ:\n" + out)

# -------- –ê–¥–º–∏–Ω-–∫–æ–º–∞–Ω–¥—ã --------
def _admin_only(update: Update) -> bool:
    return is_admin(update.effective_user.id)

async def admin_docs(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not _admin_only(update):
        await update.message.reply_text("–î–æ—Å—Ç—É–ø —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–≤.")
        return
    meta = load_docmeta()
    if not meta:
        await update.message.reply_text("–°–ø–∏—Å–æ–∫ –ø—É—Å—Ç. –í–æ–∑–º–æ–∂–Ω–æ, –µ—â—ë –Ω–∏—á–µ–≥–æ –Ω–µ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–ª–æ—Å—å –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–µ–π.\n"
                                        "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /admin_rebuild –¥–ª—è –ø–æ–ª–Ω–æ–π –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏ —Å —É—á—ë—Ç–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.")
        return
    lines = []
    for i, (h, rec) in enumerate(sorted(meta.items(), key=lambda kv: kv[1].get("time", 0)) , start=1):
        fname = rec.get("fname", "‚Äî")
        t = rec.get("time", 0)
        from datetime import datetime
        ts = datetime.fromtimestamp(t).strftime("%Y-%m-%d %H:%M") if t else "‚Äî"
        nchunks = len(rec.get("chunks", [])) if rec.get("chunks") else "?"
        lines.append(f"{i}. {fname} | chunks={nchunks} | {ts} | hash={h[:10]}...")
    idx = load_index()
    total_vec = getattr(idx, "ntotal", 0) if idx else 0
    await update.message.reply_text("–î–æ–∫—É–º–µ–Ω—Ç—ã –≤ –æ–±—É—á–µ–Ω–∏–∏:\n" + "\n".join(lines) + f"\n\n–í—Å–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {total_vec}")

async def admin_del(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not _admin_only(update):
        await update.message.reply_text("–î–æ—Å—Ç—É–ø —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–≤.")
        return
    args = context.args or []
    if not args:
        await update.message.reply_text("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: /admin_del <–Ω–æ–º–µ—Ä_–∏–∑_/admin_docs|–ø—Ä–µ—Ñ–∏–∫—Å_hash>")
        return

    target = args[0].strip()
    meta = load_docmeta()
    items = list(sorted(meta.items(), key=lambda kv: kv[1].get("time", 0)))

    # –ü–æ –Ω–æ–º–µ—Ä—É
    if target.isdigit():
        idx = int(target) - 1
        if not (0 <= idx < len(items)):
            await update.message.reply_text("–ù–µ–≤–µ—Ä–Ω—ã–π –Ω–æ–º–µ—Ä.")
            return
        file_hash, rec = items[idx]
    else:
        # –ü–æ –ø—Ä–µ—Ñ–∏–∫—Å—É hash
        matches = [(h, r) for h, r in items if h.startswith(target)]
        if not matches:
            await update.message.reply_text("–•–µ—à –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            return
        if len(matches) > 1:
            await update.message.reply_text("–ù–∞–π–¥–µ–Ω—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É ‚Äî —É—Ç–æ—á–Ω–∏—Ç–µ.")
            return
        file_hash, rec = matches[0]

    ok, msg = delete_document_from_training(file_hash, also_remove_file=False)
    await update.message.reply_text(("‚úÖ " if ok else "‚ùå ") + msg)

async def admin_del_file(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not _admin_only(update):
        await update.message.reply_text("–î–æ—Å—Ç—É–ø —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–≤.")
        return
    args = context.args or []
    if not args:
        await update.message.reply_text("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: /admin_del_file <–Ω–æ–º–µ—Ä_–∏–∑_/admin_docs|–ø—Ä–µ—Ñ–∏–∫—Å_hash>")
        return

    target = args[0].strip()
    meta = load_docmeta()
    items = list(sorted(meta.items(), key=lambda kv: kv[1].get("time", 0)))

    if target.isdigit():
        idx = int(target) - 1
        if not (0 <= idx < len(items)):
            await update.message.reply_text("–ù–µ–≤–µ—Ä–Ω—ã–π –Ω–æ–º–µ—Ä.")
            return
        file_hash, rec = items[idx]
    else:
        matches = [(h, r) for h, r in items if h.startswith(target)]
        if not matches:
            await update.message.reply_text("–•–µ—à –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            return
        if len(matches) > 1:
            await update.message.reply_text("–ù–∞–π–¥–µ–Ω—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É ‚Äî —É—Ç–æ—á–Ω–∏—Ç–µ.")
            return
        file_hash, rec = matches[0]

    ok, msg = delete_document_from_training(file_hash, also_remove_file=True)
    await update.message.reply_text(("‚úÖ " if ok else "‚ùå ") + msg)

async def admin_rebuild(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not _admin_only(update):
        await update.message.reply_text("–î–æ—Å—Ç—É–ø —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–≤.")
        return

    await update.message.reply_text("–ù–∞—á–∏–Ω–∞—é –ø–æ–ª–Ω—É—é –ø–µ—Ä–µ—Å–±–æ—Ä–∫—É –∏–Ω–¥–µ–∫—Å–∞... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è.")
    def _job():
        try:
            total, errors = full_reindex_all_documents()
            return (True, total, errors)
        except Exception as e:
            return (False, 0, [repr(e)])
    ok, total, errors = await asyncio.to_thread(_job)
    if ok:
        msg = f"–ì–æ—Ç–æ–≤–æ. –í–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ: {total}."
        if errors:
            msg += "\n–ß–∞—Å—Ç—å —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ–ø—É—â–µ–Ω–∞:\n- " + "\n- ".join(errors)
        await update.message.reply_text(msg)
    else:
        await update.message.reply_text("‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏:\n" + "\n".join(errors))

async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
    logger.exception("Unhandled error while processing update: %s", update)

# ------------ FASTAPI + PTB APP ------------
app = FastAPI()

@app.head("/")
async def health_head():
    return Response(status_code=200)

@app.get("/")
async def health_get():
    return {"ok": True}

application = ApplicationBuilder().token(TELEGRAM_BOT_TOKEN).build()
application.add_handler(CommandHandler("start", start))
application.add_handler(CommandHandler("debug", debug))
application.add_handler(CommandHandler("ai", ai_mode))
application.add_handler(CommandHandler("docs", docs_mode))
application.add_handler(CommandHandler("tm", tm_mode))
application.add_handler(CommandHandler("tm_reg", tm_cmd_reg))
application.add_handler(CommandHandler("tm_exp", tm_cmd_exp))

# Admin commands
application.add_handler(CommandHandler("admin_docs", admin_docs))
application.add_handler(CommandHandler("admin_del", admin_del))
application.add_handler(CommandHandler("admin_del_file", admin_del_file))
application.add_handler(CommandHandler("admin_rebuild", admin_rebuild))

application.add_handler(MessageHandler(filters.TEXT & filters.Regex(f"^{re.escape(AI_LABEL)}$"), ai_mode))
application.add_handler(MessageHandler(filters.TEXT & filters.Regex(f"^{re.escape(DOCS_LABEL)}$"), docs_mode))
application.add_handler(MessageHandler(filters.TEXT & filters.Regex(f"^{re.escape(TM_LABEL)}$"), tm_mode))
application.add_handler(MessageHandler(filters.Document.ALL, handle_file))
application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text))
application.add_error_handler(error_handler)

@app.on_event("startup")
async def _startup():
    await application.initialize()
    if RUN_MODE == "polling":
        await application.start()
        logger.info("PTB polling started")
    elif RUN_MODE == "webhook":
        if PUBLIC_BASE_URL:
            try:
                wh_url = f"{PUBLIC_BASE_URL}/telegram/{TELEGRAM_BOT_TOKEN}"
                await application.bot.set_webhook(url=wh_url, drop_pending_updates=True)
                logger.info("Webhook set to %s", wh_url)
            except Exception as e:
                logger.warning("Failed to set webhook: %r", e)
        logger.info("PTB initialized for webhook mode")
    logger.info("PTB initialized")

@app.on_event("shutdown")
async def _shutdown():
    if RUN_MODE == "polling":
        await application.stop()
        logger.info("PTB polling stopped")
    await application.shutdown()
    logger.info("PTB shutdown complete")

@app.post(f"/telegram/{TELEGRAM_BOT_TOKEN}")
async def telegram_webhook_token(request: Request):
    data = await request.json()
    update = Update.de_json(data, application.bot)
    await application.process_update(update)
    return {"ok": True}
